# -*- coding: utf-8 -*-
"""Wearable_MEXA_Hackathon_Gemini_Example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G1mWEAyysTCUxr0l4pwYoxt4S4ZlBZzk

# **1. Google Fit / HealthConnect API Integration**





---

Google fit beinf deactive and they are migrating t Health Connect, So best wouldbe somehwo integrate that API and use our model together with their Wearable data




Below is a code snippet to integrate Google Fit for wearable data collection. This step assumes you have access to the necessary Google Cloud credentials and a configured Google Fit API.

Code for Google Fit API Integration:
"""

# Install required libraries
!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib

from googleapiclient.discovery import build
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
import os
import pickle

# Step 1: Authenticate and Initialize Google Fit API
SCOPES = ['https://www.googleapis.com/auth/fitness.activity.read']
CLIENT_SECRET_FILE = 'path_to_your_client_secret.json'

def authenticate_google_fit():
    creds = None
    # Check if credentials already exist
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    # If no valid credentials, authenticate user
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)
            creds = flow.run_local_server(port=0)
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)
    return build('fitness', 'v1', credentials=creds)

# Step 2: Fetch Wearable Data
def get_fitness_data(service, data_type):
    # Define data sources and time range
    data_sources = service.users().dataSources().list(userId='me').execute()
    for source in data_sources.get('dataSource', []):
        if data_type in source['dataStreamName']:
            print("Data Source Found:", source['dataStreamName'])
            # Query data points
            dataset = service.users().dataSources().datasets().get(
                userId='me',
                dataSourceId=source['dataStreamId'],
                datasetId='startTime-endTime'  # Replace with appropriate timestamps
            ).execute()
            print("Data Points:", dataset.get('point', []))

# Authenticate and fetch data
service = authenticate_google_fit()
get_fitness_data(service, data_type='heart_rate')

"""# 2. **Models/Tools for Sentiment Analysis on Voice Data**
Here’s a workflow to process voice data:

Speech-to-Text Conversion:
Use Google Speech-to-Text API or AssemblyAI for high accuracy in transcription.
Sentiment Analysis:
Libraries like transformers (Hugging Face) provide pre-trained models such as BERT or RoBERTa fine-tuned for sentiment analysis.
Alternatively, OpenAI's APIs or Google Generative AI models can process text for mood classification.
Example Sentiment Analysis Code:
"""

#Example
from transformers import pipeline

# Load sentiment analysis model
sentiment_pipeline = pipeline("sentiment-analysis")

# Sample transcript
transcript = "I feel extremely excited today! Everything seems so positive and vibrant."

# Analyze sentiment
sentiment = sentiment_pipeline(transcript)
print("Sentiment Analysis Result:", sentiment)

"""# **3. Combining Wearable and Self-Reported Data**
This step integrates Google Fit data with sentiment analysis results. For now, we can simulate data as a placeholder.

Simulated Data Example:
python
Copy code

"""

import pandas as pd
from datetime import datetime, timedelta
import numpy as np

# Generate a date range
dates = [datetime.now() - timedelta(days=i) for i in range(30)]

# Simulate wearable and self-reported data
data = {
    "Date": [date.strftime('%Y-%m-%d') for date in dates],
    "Sleep Hours": np.random.uniform(3, 9, 30),
    "Activity Level": np.random.uniform(5000, 15000, 30),
    "HRV": np.random.uniform(50, 100, 30),
    "Mood": np.random.choice(["Euphoric", "Irritable", "Depressed", "Elevated"], 30),
    "EDA": np.random.uniform(0.05, 0.15, 30)
}

# Convert to DataFrame
df = pd.DataFrame(data)
print(df.head())

# Save to CSV for further use
df.to_csv('simulated_data.csv', index=False)

"""# **4. Example Test Usage**"""

# -*- coding: utf-8 -*-
"""Enhanced Bipolar Disorder Detection Pipeline"""

# Install necessary libraries (run only if required)
!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib transformers matplotlib pandas

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from transformers import pipeline


import os
from huggingface_hub import HfApi

# Get your API key from Colab secrets
hf_token = os.environ.get("HF_TOKEN")

# Initialize the Hugging Face API client
api = HfApi(token=hf_token)

# Now you can use the 'api' object to interact with the Hugging Face Hub
# ... your code to download models, datasets, etc. ...

# Helper function for rendering Markdown in Jupyter
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('•', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Step 1: Simulate Data (Wearable + Self-Reported)
dates = [datetime.now() - timedelta(days=i) for i in range(30)]
simulated_data = {
    "Date": [date.strftime('%Y-%m-%d') for date in dates],
    "Sleep Hours": np.random.uniform(3, 9, 30),
    "Activity Level": np.random.uniform(5000, 15000, 30),
    "HRV": np.random.uniform(50, 100, 30),
    "Mood": np.random.choice(["Euphoric", "Irritable", "Depressed", "Elevated"], 30),
    "EDA": np.random.uniform(0.05, 0.15, 30)
}
df = pd.DataFrame(simulated_data)
print("Simulated Dataset Preview:")
print(df.head())

# Save simulated data for further use
df.to_csv('simulated_data.csv', index=False)

# Step 2: Sentiment Analysis for Mood Logs
# Example transcript for demonstration
transcripts = [
    "I feel very tired and unmotivated today.",
    "Everything is amazing! I couldn't be happier.",
    "I feel a bit down and overwhelmed, but I'll manage.",
    "Feeling on top of the world today!",
    "I'm very anxious and can't concentrate."
]

# Load Hugging Face sentiment analysis model
sentiment_pipeline = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
sentiments = sentiment_pipeline(transcripts)

# Attach sentiments to simulated data
# Ensure transcripts has the same length as df
df['Transcript'] = (transcripts + ["Neutral"] * (len(df) - len(transcripts)))[:len(df)]
df['Sentiment'] = [sent['label'] for sent in sentiments] + ["Neutral"] * (len(df) - len(sentiments))

# Display updated data
print("\nUpdated Dataset with Sentiments:")
print(df.head())

# Step 3: Visualize Wearable Data
def plot_wearable_data(df):
    plt.figure(figsize=(12, 6))
    plt.plot(df['Date'], df['Activity Level'], label='Activity Level', marker='o')
    plt.plot(df['Date'], df['HRV'], label='HRV', marker='x')
    plt.xticks(rotation=45)
    plt.title('Activity Levels and HRV Over Time')
    plt.xlabel('Date')
    plt.ylabel('Value')
    plt.legend()
    plt.tight_layout()
    plt.show()

plot_wearable_data(df)

import pandas as pd
import matplotlib.pyplot as plt

dat = ("/content/enhanced_bipolar_analysis.csv")
df = pd.read_csv(dat)  # Read the CSV file into a Pandas DataFrame
print(df.head())       # Display the first 5 rows of the DataFrame

# Convert 'Date' column to datetime objects
df['Date'] = pd.to_datetime(df['Date'])

fig, axes = plt.subplots(3, 3, figsize=(15, 10))  # Create a 3x3 grid of subplots
fig.suptitle('Bipolar Disorder Analysis', fontsize=16)  # Set main title

# Plot each variable in a separate subplot
axes[0, 0].plot(df['Date'], df['Sleep Hours'], label='Sleep Hours', marker='o')
axes[0, 1].plot(df['Date'], df['REM Sleep (%)'], label='REM Sleep (%)', marker='o')
axes[0, 2].plot(df['Date'], df['Deep Sleep (%)'], label='Deep Sleep (%)', marker='o')
axes[1, 0].plot(df['Date'], df['Light Sleep (%)'], label='Light Sleep (%)', marker='o')
axes[1, 1].plot(df['Date'], df['Screen Time (hours)'], label='Screen Time (hours)', marker='o')
axes[1, 2].plot(df['Date'], df['Steps'], label='Steps', marker='o')
axes[2, 0].plot(df['Date'], df['Calories Burned'], label='Calories Burned', marker='o')
axes[2, 1].plot(df['Date'], df['Running Distance (km)'], label='Running Distance (km)', marker='o')
axes[2, 2].plot(df['Date'], df['Exercise Time (minutes)'], label='Exercise Time (minutes)', marker='o')


# Set labels and titles for each subplot
for ax in axes.flat:
    ax.set_xlabel('Date')
    ax.set_ylabel('Value')
    ax.legend(loc='upper left', fontsize='small')  # Adjust legend position if needed
    ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels

plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout for title and subplots
plt.show()

import google.generativeai as genai
import textwrap
from IPython.display import display, Markdown

# Helper function for rendering Markdown in Jupyter
def to_markdown(text):
    text = text.replace('•', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))


# Configure Google Generative AI API
GOOGLE_API_KEY = "AIzaSyAfyh2dwrsP2Iv2WcXEfd6bTnJinnCvDsc"  # Replace with your actual API key
genai.configure(api_key=GOOGLE_API_KEY)

# Prepare prompt
prompt_template = """
You are an expert in mental health diagnostics. Analyze the provided multimodal data to detect patterns indicative of bipolar disorder.

### Dataset Overview:
{data}

### Tasks:
1. Analyze the relationship between wearable data and mood.
2. Identify deviations that suggest manic or depressive episodes.
3. Provide risk level classifications and intervention recommendations.

### Output:
1. Overview of patterns and deviations.
2. Recommended interventions for high-risk periods.
3. Confidence level for each finding.
"""

data = ('/content/bipolar_wearable_data.csv')

# Format prompt with data
formatted_prompt = prompt_template.format(data=df.head().to_string())

# Generate insights (updated)
model = genai.GenerativeModel("gemini-1.5-pro-latest")  # Ensure model name is correct

# Pass the formatted prompt directly as a string
response = model.generate_content(formatted_prompt)

# Extract the generated insights from the response
insights = response.text

# Step 5: Display Insights
output_text = "Generated insights:\n• " + "\n• ".join(insights.split('\n'))
display(to_markdown(output_text))

# Step 6: Save Results for Further Analysis
df.to_csv('enhanced_bipolar_analysis.csv', index=False)
print("\nAnalysis saved to 'enhanced_bipolar_analysis.csv'.")







"""# Generating Bipolar data"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Generate a date range for 30 days
dates = [datetime.now() - timedelta(days=i) for i in range(30)]

# Simulating bipolar data with different phases: manic, depressive, and baseline
sleep_hours = []
sleep_rem = []
sleep_deep = []
sleep_light = []
screen_time = []
steps = []
calories = []
running_distance = []
exercise_time = []
hrv = []
eda = []
stress = []
light_exposure = []

for i in range(30):
    # Simulate bipolar phase changes (0 = baseline, 1 = manic, 2 = depressive)
    phase = np.random.choice([0, 1, 2], p=[0.4, 0.3, 0.3])  # 40% baseline, 30% manic, 30% depressive

    # Sleep patterns
    if phase == 1:  # Mania
        sleep_hours.append(np.random.uniform(3, 5))  # Reduced sleep
        sleep_rem.append(np.random.uniform(0.2, 0.4))  # More REM sleep
        sleep_deep.append(np.random.uniform(0.2, 0.3))  # Reduced deep sleep
        sleep_light.append(1 - sleep_rem[-1] - sleep_deep[-1])  # Light sleep
    elif phase == 2:  # Depression
        sleep_hours.append(np.random.uniform(8, 10))  # Increased sleep
        sleep_rem.append(np.random.uniform(0.2, 0.3))  # Reduced REM sleep
        sleep_deep.append(np.random.uniform(0.3, 0.4))  # More deep sleep
        sleep_light.append(1 - sleep_rem[-1] - sleep_deep[-1])  # Light sleep
    else:  # Baseline
        sleep_hours.append(np.random.uniform(6, 8))  # Normal sleep
        sleep_rem.append(np.random.uniform(0.2, 0.3))  # Normal REM sleep
        sleep_deep.append(np.random.uniform(0.2, 0.3))  # Normal deep sleep
        sleep_light.append(1 - sleep_rem[-1] - sleep_deep[-1])  # Normal light sleep

    # Screen time
    screen_time.append(np.random.uniform(2, 6) if phase != 2 else np.random.uniform(3, 8))  # More screen time during depression

    # Vitality data (steps, calories, exercise)
    if phase == 1:  # Mania
        steps.append(np.random.randint(10000, 20000))  # Very high activity levels
        calories.append(steps[-1] * 0.04)  # High calorie burn
        running_distance.append(np.random.uniform(5, 10))  # High running distance
        exercise_time.append(np.random.uniform(60, 120))  # High exercise time
    elif phase == 2:  # Depression
        steps.append(np.random.randint(2000, 5000))  # Very low activity
        calories.append(steps[-1] * 0.04)  # Low calorie burn
        running_distance.append(np.random.uniform(0, 1))  # Minimal running
        exercise_time.append(np.random.uniform(10, 30))  # Low exercise time
    else:  # Baseline
        steps.append(np.random.randint(6000, 10000))  # Normal activity levels
        calories.append(steps[-1] * 0.04)  # Normal calorie burn
        running_distance.append(np.random.uniform(2, 5))  # Moderate running distance
        exercise_time.append(np.random.uniform(30, 60))  # Normal exercise time

    # HRV, EDA, and Stress
    if phase == 1:  # Mania
        hrv.append(np.random.uniform(50, 80))  # Elevated HRV
        eda.append(np.random.uniform(0.1, 0.3))  # Elevated EDA (stress)
        stress.append(np.random.uniform(50, 100))  # High stress
    elif phase == 2:  # Depression
        hrv.append(np.random.uniform(30, 50))  # Low HRV
        eda.append(np.random.uniform(0.05, 0.1))  # Low EDA
        stress.append(np.random.uniform(30, 60))  # Moderate stress
    else:  # Baseline
        hrv.append(np.random.uniform(60, 75))  # Normal HRV
        eda.append(np.random.uniform(0.05, 0.2))  # Normal EDA
        stress.append(np.random.uniform(20, 40))  # Low stress

    # Light Exposure (Lux)
    if phase == 1:  # Mania
        light_exposure.append(np.random.uniform(200, 3000))  # Disrupted circadian rhythm
    elif phase == 2:  # Depression
        light_exposure.append(np.random.uniform(100, 1000))  # Low exposure
    else:  # Baseline
        light_exposure.append(np.random.uniform(500, 2000))  # Normal exposure

# Create a DataFrame with all the simulated bipolar data
data = {
    "Date": [date.strftime('%Y-%m-%d') for date in dates],
    "Sleep Hours": sleep_hours,
    "REM Sleep (%)": [x * 100 for x in sleep_rem], # Fix: Multiply each element, not the list
    "Deep Sleep (%)": [x * 100 for x in sleep_deep], # Fix: Multiply each element, not the list
    "Light Sleep (%)": [x * 100 for x in sleep_light], # Fix: Multiply each element, not the list
    "Screen Time (hours)": screen_time,
    "Steps": steps,
    "Calories Burned": calories,
    "Running Distance (km)": running_distance,
    "Exercise Time (minutes)": exercise_time,
    "HRV": hrv,
    "EDA": eda,
    "Stress Level": stress,
    "Light Exposure (lux)": light_exposure
}

df = pd.DataFrame(data)

# Show the first few rows of the simulated data
print(df.head())

# Save to CSV for further use
df.to_csv('bipolar_wearable_data.csv', index=False)

# Optionally, display basic statistics for each parameter
print("\nBasic Statistics:")
print(df.describe())

"""# **Generating Euthymic data**"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Generate a date range for 30 days
dates = [datetime.now() - timedelta(days=i) for i in range(30)]

# Initialize lists to hold simulated data
sleep_hours = []
sleep_rem = []
sleep_deep = []
sleep_light = []
screen_time = []
steps = []
calories = []
running_distance = []
exercise_time = []
hrv = []
eda = []
stress = []
light_exposure = []

# Simulate consistent and balanced data for an euthymic person
for _ in range(30):
    # Sleep patterns
    sleep_hours.append(np.random.uniform(6.5, 8.5))  # Normal range of sleep hours
    sleep_rem.append(np.random.uniform(0.2, 0.25))  # Normal REM sleep proportion
    sleep_deep.append(np.random.uniform(0.2, 0.25))  # Normal deep sleep proportion
    sleep_light.append(1 - sleep_rem[-1] - sleep_deep[-1])  # Remaining proportion as light sleep

    # Screen time
    screen_time.append(np.random.uniform(2.5, 4.5))  # Moderate screen time

    # Vitality data
    steps.append(np.random.randint(7000, 12000))  # Moderate daily steps
    calories.append(steps[-1] * 0.045)  # Calories burned based on activity level
    running_distance.append(np.random.uniform(1, 3))  # Moderate running distance
    exercise_time.append(np.random.uniform(30, 60))  # Moderate exercise time in minutes

    # HRV, EDA, and Stress
    hrv.append(np.random.uniform(60, 80))  # Healthy HRV range
    eda.append(np.random.uniform(0.05, 0.15))  # Low to moderate EDA (relaxed state)
    stress.append(np.random.uniform(20, 40))  # Low to moderate stress level

    # Light Exposure
    light_exposure.append(np.random.uniform(500, 1500))  # Normal light exposure for balanced circadian rhythm

# Create a DataFrame with all the simulated euthymic data
data = {
    "Date": [date.strftime('%Y-%m-%d') for date in dates],
    "Sleep Hours": sleep_hours,
    "REM Sleep (%)": [rem * 100 for rem in sleep_rem],
    "Deep Sleep (%)": [deep * 100 for deep in sleep_deep],
    "Light Sleep (%)": [light * 100 for light in sleep_light],
    "Screen Time (hours)": screen_time,
    "Steps": steps,
    "Calories Burned": calories,
    "Running Distance (km)": running_distance,
    "Exercise Time (minutes)": exercise_time,
    "HRV": hrv,
    "EDA": eda,
    "Stress Level": stress,
    "Light Exposure (lux)": light_exposure
}

df_euthymic = pd.DataFrame(data)

# Show the first few rows of the simulated data
print("Euthymic Person's Wearable Data:")
print(df_euthymic.head())

# Save to CSV for further use
df_euthymic.to_csv('euthymic_wearable_data.csv', index=False)

# Optionally, display basic statistics for each parameter
print("\nBasic Statistics for Euthymic Data:")
print(df_euthymic.describe())

import pandas as pd
import matplotlib.pyplot as plt

dat = ("/content/euthymic_wearable_data.csv")
df = pd.read_csv(dat)  # Read the CSV file into a Pandas DataFrame
print(df.head())       # Display the first 5 rows of the DataFrame

# Convert 'Date' column to datetime objects
df['Date'] = pd.to_datetime(df['Date'])

fig, axes = plt.subplots(3, 3, figsize=(15, 10))  # Create a 3x3 grid of subplots
fig.suptitle('Euthymic Analysis', fontsize=16)  # Set main title

# Plot each variable in a separate subplot
axes[0, 0].plot(df['Date'], df['Sleep Hours'], label='Sleep Hours', marker='o')
axes[0, 1].plot(df['Date'], df['REM Sleep (%)'], label='REM Sleep (%)', marker='o')
axes[0, 2].plot(df['Date'], df['Deep Sleep (%)'], label='Deep Sleep (%)', marker='o')
axes[1, 0].plot(df['Date'], df['Light Sleep (%)'], label='Light Sleep (%)', marker='o')
axes[1, 1].plot(df['Date'], df['Screen Time (hours)'], label='Screen Time (hours)', marker='o')
axes[1, 2].plot(df['Date'], df['Steps'], label='Steps', marker='o')
axes[2, 0].plot(df['Date'], df['Calories Burned'], label='Calories Burned', marker='o')
axes[2, 1].plot(df['Date'], df['Running Distance (km)'], label='Running Distance (km)', marker='o')
axes[2, 2].plot(df['Date'], df['Exercise Time (minutes)'], label='Exercise Time (minutes)', marker='o')


# Set labels and titles for each subplot
for ax in axes.flat:
    ax.set_xlabel('Date')
    ax.set_ylabel('Value')
    ax.legend(loc='upper left', fontsize='small')  # Adjust legend position if needed
    ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels

plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout for title and subplots
plt.show()







"""# **OLD Version**"""

# Load the datasets
unstable_data = pd.read_csv('Unstable_Lifestyle_Dataset.csv')
stable_data = pd.read_csv('Wearable_Dataset_Example.csv')

# Convert 'Date' and 'Time' to datetime objects in unstable_data
unstable_data['DateTime'] = pd.to_datetime(unstable_data['Date'] + ' ' + unstable_data['Time'])
unstable_data = unstable_data.drop(columns=['Date', 'Time'])

# Display the first few rows of the datasets
print("Unstable Lifestyle Dataset:")
print(unstable_data.head())
print("\nStable Wearable Dataset:")
print(stable_data.head())

# Feature extraction from stable wearable data (example)
if 'Steps' in stable_data.columns and 'Calories Burned' in stable_data.columns:
    stable_data['Activity_Level'] = stable_data['Steps'] / stable_data['Calories Burned']
if 'Sleep Stage' in stable_data.columns and 'HRV (ms)' in stable_data.columns:
    stable_data['Sleep_Efficiency'] = stable_data['HRV (ms)'] / stable_data['Sleep Stage'].apply(lambda x: 1 if x == 'Deep' else 0.5)

# Display the modified stable wearable dataset
print("\nModified Stable Wearable Dataset:")
print(stable_data.head())

# Configure genai to use your API Key
GOOGLE_API_KEY = 'AIzaSyAfyh2dwrsP2Iv2WcXEfd6bTnJinnCvDsc'  # Replace with your actual API key
genai.configure(api_key=GOOGLE_API_KEY)

"""
# List the available gemini models
for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)
"""
# Select a model and instantiate a GenerativeModel
model = genai.GenerativeModel('gemini-1.5-pro-latest')

# Select a model and instantiate a GenerativeModel
model = genai.GenerativeModel('gemini-1.5-pro-latest')

# Write a prompt for Gemini to read and analyze the unstable lifestyle data
prompt_unstable = """
Analyze the following unstable lifestyle data to detect patterns and provide feedback on possible lifestyle changes for better mental health and well-being.

Unstable Lifestyle Data:
{}

Provide detailed feedback and suggestions for improvement.
""".format(unstable_data.to_string())

# Generate insights using the model
response_unstable = model.generate_content(prompt_unstable)
insights_unstable = response_unstable.text
print(insights_unstable)

# Render output to Markdown
output_text_unstable = "Generated insights for Unstable Lifestyle Data:\n• " + "\n• ".join(insights_unstable.split('\n'))
display(to_markdown(output_text_unstable))

# Write a prompt for Gemini to read and analyze the stable wearable data
prompt_stable = """
Analyze the following stable wearable device data to detect patterns and provide feedback on possible lifestyle changes for better mental health and well-being.

Stable Wearable Device Data:
{}

Provide detailed feedback and suggestions for improvement.
""".format(stable_data.to_string())

# Generate insights using the model
response_stable = model.generate_content(prompt_stable)
insights_stable = response_stable.text
print(insights_stable)

# Render output to Markdown
output_text_stable = "Generated insights for Stable Wearable Data:\n• " + "\n• ".join(insights_stable.split('\n'))
display(to_markdown(output_text_stable))

# Add an image to files, on the left menu and generate text from the image
# Import Image packages
import PIL.Image

# Change the name here to the name of the file you uploaded
img = PIL.Image.open('/content/1733324260733.jpg')
img

# Select a model and instantiate a GenerativeModel
model = genai.GenerativeModel('gemini-1.5-flash')

# Pass in text and the image to generate an output
prompt = """
Analyze the following handwriting image to detect potential mental health problems. Provide detailed feedback and suggestions for improvement based on the analysis.

Handwriting Image:
"""

response = model.generate_content([prompt, img], stream=True)
response.resolve()

# Display the response
print(response.text)



